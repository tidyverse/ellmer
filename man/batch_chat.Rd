% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/batch-chat.R
\name{batch_chat}
\alias{batch_chat}
\title{Submit multiple chats in one batch}
\usage{
batch_chat(chat, prompts, path, wait = TRUE)
}
\arguments{
\item{path}{Path to file (with \code{.json} extension) to store state.}

\item{wait}{If \code{TRUE}, will wait for batch to complete. If \code{FALSE},
it will check once and error if the job is not complete.}

\item{inheritParams}{parallel_chat}
}
\description{
\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}}

Some providers (i.e. OpenAI and anthropic) support a batched API where you
can submit many requests at once, and you get the results back within 24
hours. If you want to get results back more quickly, you may want to use
\code{\link[=parallel_chat]{parallel_chat()}} instead.

Batched requests are cheaper (usually 50\% off), but ellmer does not yet
reflect this discount in its pricing.

Since batched requests can take up to 24 hours to complete, \code{batch_chat()}
is designed to be resumable. You can either set \code{wait = FALSE} or simply
interrupt the waiting process, then later, call \code{batch_chat()} to resume
the process. It's up to you to delete the file once you're done with
it; ellmer will not delete it for you.

This API is marked as experimental, since I don't know how to helpfully
deal with errors. Fortunately they don't seem to be common, but if you
have ideas, please let me know!
}
\examples{
chat <- chat_openai()

# Chat ----------------------------------------------------------------------
country <- c("Canada", "New Zealand", "Jamaica", "United States")
prompts <- interpolate("What's the capital of {{country}}?")
chats <- batch_chat(chat, prompts, path = "capitals.json")
chats
}
