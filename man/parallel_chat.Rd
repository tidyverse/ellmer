% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/chat-parallel.R
\name{parallel_chat}
\alias{parallel_chat}
\title{Submit multiple chats in parallel}
\usage{
parallel_chat(chat, prompts, max_active = 10, rpm = 500)
}
\arguments{
\item{chat}{A base chat object.}

\item{prompts}{A list of user prompts.}

\item{max_active}{The maximum number of simultaneous requests to send.

For \code{\link[=chat_anthropic]{chat_anthropic()}}, note that the number of active connections is
limited primarily by the output tokens per minute limit (OTPM) which is
estimated from the \code{max_tokens} parameter, which defaults to 4096. That
means if your usage tier limits you to 16,000 OTPM, you should either set
\code{max_active = 4} (16,000 / 4096) to decrease the number of active
connections or use \code{\link[=params]{params()}} in \code{chat_anthropic()} to decrease
\code{max_tokens}.}

\item{rpm}{Maximum number of requests per minute.}
}
\value{
A list of \link{Chat} objects, one for each prompt.
}
\description{
\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}}

If you have multiple prompts, you can submit them in parallel. This is
typically considerably faster if you have many prompts to evaluate.
}
\examples{
\dontshow{if (ellmer::has_credentials("openai")) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
country <- c("Canada", "New Zealand", "Jamaica", "United States")
prompts <- interpolate("What's the capital of {{country}}?")

chat <- chat_openai()
parallel_chat(chat, prompts)
\dontshow{\}) # examplesIf}
}
